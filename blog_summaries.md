# Interesting blogs and articles from across the interwebs
[Personal Blogs](blog_summaries.md#personal-blogs)  
[Company Blogs](blog_summaries.md#company-blogs)  
[Random Articles](blog_summaries.md#random-articles)  

### Personal Blogs

- [Ed Chen](http://blog.echen.me/)
  Every post contains an incredibly clear explanation of a complicated topic in ML. For example, [causal inference techniques](http://blog.echen.me/2014/08/15/propensity-modeling-causal-inference-and-discovering-drivers-of-growth/), [summary of Netflix prize-winning techniques](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/), and [gaussian mixture modeling](http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/). Unfortunately, not updated recently.

- [Alex Madrigal](https://www.theatlantic.com/author/alexis-madrigal/)
  One of my favorite journalists in tech. Some highlights are [Reverse Engineering Netflix](https://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/) and [What is Pinterest? A Database of Intentions](https://www.theatlantic.com/technology/archive/2014/07/what-is-pinterest-a-database-of-intentions/375365/)

- [Benedict Evans](https://www.ben-evans.com/archive/)
  Every article is an insightful, opinionated view of some aspect of the tech industry, and where it's heading.

- [Justin O'Beirne](https://www.justinobeirne.com/)
  A brilliant running series that analyzes the differences between Apple and Google Maps, and illustrates in fascinating detail how Google Maps has consistently stayed 1-2 steps ahead of Apple.

- [Colah's blog](http://colah.github.io/)
  Clear explanations (and great diagrams) of many topics in neural networks.

- [Andrew Gelman](https://statmodeling.stat.columbia.edu/)
  Stats profession and expert in Bayesian methods. Frequently updated. He frequently makes posts about statistics applied poorly and I especially enjoyed the one on bad uses of [regression discontinuity](https://statmodeling.stat.columbia.edu/2018/08/02/38160/) (4th order polynomial fits!). Another post on [why well-designed randomized studies fail to generalize](https://statmodeling.stat.columbia.edu/2018/11/22/also-observed-results-smaller-studies-conducted-ngos-often-pilot-studies-often-look-promising-governments-tried-implement-scaled-versions/) when the treatment is applied more broadly is also interesting.

- [Flowing Data (Nathan Yau)](https://flowingdata.com/)
  Tons of great visualization examples. In general, anything you might want to know about data visualization can probably be found here.

- [xkcd - Randall Munroe](https://xkcd.com/)

### Company Blogs
The blogs below are all worth following. For each one, I've highlighted the articles that I found particularly interesting.

- [Airbnb](https://medium.com/airbnb-engineering/tagged/data-science)
  - [Learning market dynamics for optimal pricing](https://medium.com/airbnb-engineering/learning-market-dynamics-for-optimal-pricing-97cffbcc53e3) is a very interesting post because it solves a problem by combining a structural approach with ML. The reason a structural approach (model-based) was useful was because of a few reasons: 1) they wanted a prediction distribution, 2) data was sparse, and 3) scale. Furthermore, and most importantly, the data showed that the demand curves shared the same shape (exponential with cyclical patterns). So essentially, they made a strong (prior) assumption of what the functional form should take, then use machine learning to find the parameters of the function. This solves all 3 of the above problems because the model inherently provides prediction distributions and each model only has a small number of parameters.
  - [Listing Embeddings](https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e). Yet another example of the power of embeddings to extract (low) dimensional representations of categorical variables, which can be used to extract relationships and similarities between categories (word vectors are the first, and most popular example).
  - [ML powered search ranking of airbnb experiences](https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789) has a surprising amount of details on what it takes to build (and explain and monitor) a production ML model. The post details each stage of their process, showing how they slowly layered on new ideas and validated each stage.

- [StichFix](https://multithreaded.stitchfix.com/)
  - [Mixed-effect models](https://multithreaded.stitchfix.com/blog/2015/07/14/glmms/). I haven't yet used this method, but it looks quite useful. It seems similar to how the BellKor team won the [Netflix competition](papers/The%20BellKor%20Solution%20to%20the%20netflix%20grand%20prize.pdf), but with an (Empirical) Bayesian twist. The BellKor team built a model that separated out mean effects from personal effects from time-varying effects, and more. Mixed-effect models seem to take a similar approach, but treat each term as a random variable, the distributions of which are determined by the data.
  - [So, You Need a Statistically Significant Sample?](https://multithreaded.stitchfix.com/blog/2015/05/26/significant-sample/). A good overview of power analysis.
  - [May Bayes Theorum Be With You](https://multithreaded.stitchfix.com/blog/2015/02/12/may-bayes-theorem-be-with-you/). Why Bayesian techniques might be useful. In the Frequentist paradigm, the parameters are fixed (e.g. mean, var) but unknown, and the data is random. In the Bayesian approach, the data is fixed and the parameters are random. Credible intervals (Bayesian) give us the probability that the parameter lives in that interval, as opposed to confidence intervals (Frequentist) which tell us the proportion of times a parameter will fall in the interval if we ran the experiment many times. At StichFix, they use both. Frequentists methods for clinically designed A/B tests, Bayesian methods for when they analyze less randomized data.
  - [Time dependent classification](https://multithreaded.stitchfix.com/blog/2017/09/08/time-dependent-classification/). How do you deal with covariates/features that change over time? Some methods "are Bayesian dynamic models (aka "state space" models), and random effects models". This post talks about a time-varying logistic regression model with regularization that has been found useful at StitchFix.

- [Google](http://www.unofficialgoogledatascience.com/)
  - [Designing A/B Tests in a Collaboration Network](http://www.unofficialgoogledatascience.com/2018/01/designing-ab-tests-in-collaboration.html). A good practical example on how to design and analyze experiments on a network.
  - [Using Random Effect Models in Prediction Problems](http://www.unofficialgoogledatascience.com/2016/03/using-random-effects-models-in.html). This post argues for the usefulness of random effect models as another tool that ML practitioners should have in their toolbox. "Random effect models can be viewed as an application of empirical Bayes" and can have superior performance compared to fixed effect models. To me, one of the biggest advantage of random effect models is that you obtain an estimate of your prediction uncertainty, the predictive posterior distribution.
  - [Compliance bias in mobile experiments](http://www.unofficialgoogledatascience.com/2018/03/quicker-decisions-in-imperfect-mobile.html). Discussion about dealing with compliance bias, when uptake rate is not uniform (for instance, people take time to download a new version of an app, or people's likelihood of taking a drug depends on the severity of their symptoms). In additional to the binary variable that indicates whether a person is assigned to treatment vs control, another binary variable is needed to capture whether that person has experienced the treatment. An additional complication is selection bias, since uptake rate is not random, but could depend on properties of the sample (poor connectivity, hardware quality, etc.). They show that 2 simple analysis strategies, "intent to treat" and "treatment on the treated", both have poor results (both strategies look at only one of the binary variables at a time). Instead, they suggest looking at the potential outcomes of both binary variables at the same time, and then use propensity matching (either within the treatment or within the control). What is interesting is when you bucket by propensity score, one can see that the treatment affects different buckets differently.
  - [Causality in ML](http://www.unofficialgoogledatascience.com/2017/01/causality-in-machine-learning.html). This is an interesting post on google's attempts to deal with selection bias and feedbacks between selection and their labels. In their case, the feedback between ranking items ("prominence") and measuring relevance to a user ("quality"). In the case of credit scoring, the problem is slightly simpler, since we don't have to deal with feedback like the case of the top link being more likely to be clicked on because of the location, but we still have to deal with the problem of training a model on the factual distribution, which doesn't match the overall distribution.

    The solution they have ended up with, that they say works well in practice, is to not treat the data the same, but rather explicitly handle separately the factual examples vs. the counterfactual examples. Then build a iterative model that alternates between holding the factual model coefficients constant vs. the counterfactual model coefficients constant. This allows them to estimate the impact of the selection bias.

    The best quotes are in the last section titled "Using randomization in training".

    "These systems don't classically overfit … but they tend to overfit to the observed distribution of the data. As we described earlier, this factual distribution doesn't match the counterfactual distribution of data and hence the model can fail to generalize."

    "As we observed at the start of this post, standard machine learning techniques don't distinguish between randomized and observational data the way statistical models do. To make better estimates, we need the randomized data to play a different role than the observational data in model training."
  - [To balance or not to balance?](http://www.unofficialgoogledatascience.com/2016/06/to-balance-or-not-to-balance.html). The intro says it all, and so I'll quote directly. "Determining the causal effects of an action—which we call treatment—on an outcome of interest is at the heart of many data analysis efforts. In an ideal world, experimentation through randomization of the treatment assignment allows the identification and consistent estimation of causal effects. In observational studies treatment is assigned by nature, therefore its mechanism is unknown and needs to be estimated. This can be done through estimation of a quantity known as the propensity score, defined as the probability of receiving treatment within strata of the observed covariates. There are two types of estimation method for propensity scores.  The first tries to predict treatment as accurately as possible.  The second tries to balance the distribution of predictors evenly between the treatment and control groups. The two approaches are related, because different predictor values among treated and control units could be used to better predict treatment status. In this post we discuss issues related to these goals, specification of loss functions for the two objectives, and compare both methods via simulation.". Their result is that unless one has domain-expertise, the better approach is to not balance, to prioritize the accuracy of the propensity score.

- [Uber](https://eng.uber.com/tag/data-science/)
  - [Databook](https://eng.uber.com/databook/). Documentation isn't sexy, but it's extremely important. This applies to code, but maybe even more importantly to data. Proper documentation allows anyone working with data to work faster since they can easily understand the data, they can trust the data, be notified if the data changes, and can make changes knowing exactly who will be affected upstream.
  - [Engineering Next-Gen Location at Uber](https://eng.uber.com/rethinking-gps/). GPS can be inaccurate in urban environments. They suggest algorithms to improve the accuracy of their location estimates using multiple methods (signal strength, probabilistic shadow matching, and then combining the predictions using particle filters).

- [No Free Hunch](http://blog.kaggle.com/). Kaggle's blog where they post many interviews from competition winners, which are a great way to quickly learn about techniques which worked (and didn't) for a particular problem.

### Random Articles
- [Winning solutions of kaggle competitions](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions). A notebook that aggregates forum posts and winner's interviews from all past Kaggle competitions. A great way to keep track of state-of-the-art public techniques.
- [Understanding empirical Bayes estimates (using baseball statistics)](http://varianceexplained.org/r/empirical_bayes_baseball/). A clear introduction into empirical Bayes and why it's useful.
- [scikit-learn-contrib](https://github.com/scikit-learn-contrib). A github repo that contains scikit-learn compatible projects. Some highlights are boruta (feature selection), imbalanced-learn, and categorical-encoding.
- [Monotonicity constraints in ML](http://blog.datadive.net/monotonicity-constraints-in-machine-learning/). If one has a strong prior belief that a feature should have a monotonic relationship with the target, adding monotonicity constraints can help a model generalize better (reduces overfitting).
- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/). A warning that if your categorical variables are high-dimensional and you combine them into a single model with continuous features, that decision trees will tend to split on the continuous features. This is true even if the categorical features are very strong. The blog post doesn't suggest solutions aside from suggesting logistic regression, but if one wants to use decision tree approaches, one can either make two separate models or find a way to densify the categorical features (target encoding, entity embedding).
- [Reflecting on one very, very strange year at Uber](https://www.susanjfowler.com/blog/2017/2/19/reflecting-on-one-very-strange-year-at-uber). Susan Fowler's post was one of the catalysts of the #MeToo
movement. It also kickstarted the backlash against Uber, resulting in a series of negative stories about Uber, culminating in the resignation of Travis Kalanick, and then also [spreading](https://www.wired.com/story/the-other-tech-bubble/) to "big tech" in general.
- [WeChat in China](https://a16z.com/2015/08/06/wechat-china-mobile-first/). A classic article on how WeChat built its superapp ecosystem, and why it's so important (web vs. mobile, payments, distribution power, offline-to-online, social).
- [The ethical dilemma facing silicon valley's next generation](https://www.theringer.com/tech/2019/2/6/18212421/stanford-students-tech-backlash-silicon-valley-next-generation). Tackles the issue of where to work if one has the choice to work at a tech giant. Is Silicon Valley the west coast version of Wall Street, selling their souls for money? A quote that resonated with me was the following from Alex Stamos, former chief security officer at FB, "He tells them that they should, not in spite of the companies’ mounting issues, but because of them. “If you actually care about making communication technologies compatible with democracy, then the place to be is at one of the companies that actually has the problems,” he says. “Not working at the big places that could actually solve it does not make things better.”".
- [Why do we work so hard? - Economist](https://www.1843magazine.com/features/why-do-we-work-so-hard). A pondering on the nature of work, and what it means. What gives us identity, community, purpose? Is it work? If yes, is it healthy for work and social life to be so intertwined? What does that say about us? Even if work is fun, why should we do so much of it?
